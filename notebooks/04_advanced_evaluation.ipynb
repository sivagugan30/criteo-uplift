{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced Uplift Model Evaluation\n",
        "\n",
        "This notebook covers advanced evaluation techniques for uplift models:\n",
        "\n",
        "1. **Uplift by Decile Analysis** - Granular view of model performance across population segments\n",
        "2. **Calibration Plot for Uplift** - Are predicted uplifts well-calibrated?\n",
        "3. **Statistical Significance Testing** - Bootstrap hypothesis tests to compare models\n",
        "\n",
        "These go beyond basic metrics (Qini, AUUC) to provide actionable business insights and statistical rigor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Libraries loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBClassifier, XGBRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# CausalML\n",
        "from causalml.inference.meta import BaseTClassifier, BaseSClassifier, BaseXClassifier\n",
        "\n",
        "# Plot settings\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 11\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "print(\"âœ… Libraries loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Data and Train Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training set: 210,000 rows\n",
            "Test set: 90,000 rows\n",
            "Test conversion rate: 0.312%\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "df = pd.read_parquet('../data/raw/criteo_uplift.parquet')\n",
        "\n",
        "# Sample for training\n",
        "SAMPLE_SIZE = 300_000\n",
        "df_sample = df.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Define features and targets\n",
        "feature_cols = [f'f{i}' for i in range(12)]\n",
        "X = df_sample[feature_cols].values\n",
        "y = df_sample['conversion'].values\n",
        "treatment = df_sample['treatment'].values\n",
        "\n",
        "# Train/Test split\n",
        "X_train, X_test, y_train, y_test, t_train, t_test = train_test_split(\n",
        "    X, y, treatment, \n",
        "    test_size=0.3, \n",
        "    random_state=42,\n",
        "    stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set: {len(X_train):,} rows\")\n",
        "print(f\"Test set: {len(X_test):,} rows\")\n",
        "print(f\"Test conversion rate: {y_test.mean()*100:.3f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train T-Learner, S-Learner, X-Learner\n",
        "print(\"Training uplift models...\")\n",
        "\n",
        "# T-Learner\n",
        "learner_t = BaseTClassifier(learner=XGBClassifier(\n",
        "    n_estimators=100, max_depth=5, learning_rate=0.1, \n",
        "    random_state=42, eval_metric='logloss'\n",
        "))\n",
        "learner_t.fit(X=X_train, treatment=t_train, y=y_train)\n",
        "uplift_t = learner_t.predict(X=X_test).flatten()\n",
        "print(\"âœ… T-Learner trained\")\n",
        "\n",
        "# S-Learner\n",
        "learner_s = BaseSClassifier(learner=XGBClassifier(\n",
        "    n_estimators=100, max_depth=5, learning_rate=0.1,\n",
        "    random_state=42, eval_metric='logloss'\n",
        "))\n",
        "learner_s.fit(X=X_train, treatment=t_train, y=y_train)\n",
        "uplift_s = learner_s.predict(X=X_test).flatten()\n",
        "print(\"âœ… S-Learner trained\")\n",
        "\n",
        "# X-Learner\n",
        "learner_x = BaseXClassifier(\n",
        "    outcome_learner=XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42, eval_metric='logloss'),\n",
        "    effect_learner=XGBRegressor(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42)\n",
        ")\n",
        "learner_x.fit(X=X_train, treatment=t_train, y=y_train)\n",
        "uplift_x = learner_x.predict(X=X_test).flatten()\n",
        "print(\"âœ… X-Learner trained\")\n",
        "\n",
        "# Store predictions\n",
        "models = {\n",
        "    'T-Learner': uplift_t,\n",
        "    'S-Learner': uplift_s,\n",
        "    'X-Learner': uplift_x\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Uplift by Decile Analysis\n",
        "\n",
        "**Why Decile Analysis?**\n",
        "- More granular than overall metrics\n",
        "- Shows uplift in each 10% segment of population\n",
        "- Answers: \"If I target top 30%, what uplift do I get in each decile?\"\n",
        "- Reveals if model truly separates Persuadables from others\n",
        "\n",
        "**Columns Explained:**\n",
        "- **Decile**: Population segment ranked by predicted uplift (1 = highest predicted)\n",
        "- **Avg Predicted Uplift**: Mean predicted CATE in that decile\n",
        "- **Actual Uplift**: Observed uplift (treatment conv rate - control conv rate)\n",
        "- **Lift vs Random**: How much better than random targeting?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uplift_by_decile(y_true, uplift_pred, treatment, n_deciles=10):\n",
        "    \"\"\"\n",
        "    Calculate uplift metrics by decile.\n",
        "    \n",
        "    Returns DataFrame with:\n",
        "    - Decile (1 = highest predicted uplift)\n",
        "    - Avg Predicted Uplift\n",
        "    - Actual Uplift (treatment effect in that decile)\n",
        "    - Sample sizes\n",
        "    - Lift vs Random\n",
        "    \"\"\"\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'y': y_true,\n",
        "        'uplift_pred': uplift_pred,\n",
        "        'treatment': treatment\n",
        "    })\n",
        "    \n",
        "    # Rank by predicted uplift (descending) and assign deciles\n",
        "    df['decile'] = pd.qcut(df['uplift_pred'].rank(method='first'), \n",
        "                           q=n_deciles, labels=False)\n",
        "    df['decile'] = n_deciles - df['decile']  # So decile 1 = highest uplift\n",
        "    \n",
        "    # Calculate overall (random) uplift\n",
        "    overall_uplift = (df[df['treatment']==1]['y'].mean() - \n",
        "                      df[df['treatment']==0]['y'].mean())\n",
        "    \n",
        "    results = []\n",
        "    for decile in range(1, n_deciles + 1):\n",
        "        subset = df[df['decile'] == decile]\n",
        "        \n",
        "        treated = subset[subset['treatment'] == 1]\n",
        "        control = subset[subset['treatment'] == 0]\n",
        "        \n",
        "        avg_pred_uplift = subset['uplift_pred'].mean()\n",
        "        \n",
        "        if len(treated) > 0 and len(control) > 0:\n",
        "            actual_uplift = treated['y'].mean() - control['y'].mean()\n",
        "        else:\n",
        "            actual_uplift = np.nan\n",
        "            \n",
        "        lift_vs_random = actual_uplift / overall_uplift if overall_uplift != 0 else np.nan\n",
        "        \n",
        "        results.append({\n",
        "            'Decile': decile,\n",
        "            'N': len(subset),\n",
        "            'N_Treated': len(treated),\n",
        "            'N_Control': len(control),\n",
        "            'Avg Predicted Uplift (%)': avg_pred_uplift * 100,\n",
        "            'Actual Uplift (%)': actual_uplift * 100,\n",
        "            'Lift vs Random': lift_vs_random\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"âœ… Decile analysis function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate decile analysis for each model\n",
        "print(\"=\" * 80)\n",
        "print(\"UPLIFT BY DECILE ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for model_name, uplift_pred in models.items():\n",
        "    print(f\"\\n{'='*40}\")\n",
        "    print(f\"ðŸ“Š {model_name}\")\n",
        "    print(f\"{'='*40}\")\n",
        "    \n",
        "    decile_df = uplift_by_decile(y_test, uplift_pred, t_test)\n",
        "    \n",
        "    # Format for display\n",
        "    display_df = decile_df.copy()\n",
        "    display_df['Avg Predicted Uplift (%)'] = display_df['Avg Predicted Uplift (%)'].round(4)\n",
        "    display_df['Actual Uplift (%)'] = display_df['Actual Uplift (%)'].round(4)\n",
        "    display_df['Lift vs Random'] = display_df['Lift vs Random'].round(2)\n",
        "    \n",
        "    print(display_df.to_string(index=False))\n",
        "    \n",
        "    # Key insight\n",
        "    top_decile_lift = decile_df.iloc[0]['Lift vs Random']\n",
        "    print(f\"\\nðŸ’¡ Top decile delivers {top_decile_lift:.1f}x the uplift of random targeting\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize decile analysis\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "colors = {'T-Learner': '#3498db', 'S-Learner': '#e74c3c', 'X-Learner': '#2ecc71'}\n",
        "\n",
        "for ax, (model_name, uplift_pred) in zip(axes, models.items()):\n",
        "    decile_df = uplift_by_decile(y_test, uplift_pred, t_test)\n",
        "    \n",
        "    x = decile_df['Decile']\n",
        "    \n",
        "    # Bar chart: Predicted vs Actual\n",
        "    width = 0.35\n",
        "    ax.bar(x - width/2, decile_df['Avg Predicted Uplift (%)'], width, \n",
        "           label='Predicted', color=colors[model_name], alpha=0.7)\n",
        "    ax.bar(x + width/2, decile_df['Actual Uplift (%)'], width,\n",
        "           label='Actual', color='gray', alpha=0.7)\n",
        "    \n",
        "    ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
        "    ax.set_xlabel('Decile (1 = Highest Predicted Uplift)')\n",
        "    ax.set_ylabel('Uplift (%)')\n",
        "    ax.set_title(f'{model_name}: Predicted vs Actual Uplift')\n",
        "    ax.set_xticks(x)\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models: Lift vs Random by Decile\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "for model_name, uplift_pred in models.items():\n",
        "    decile_df = uplift_by_decile(y_test, uplift_pred, t_test)\n",
        "    ax.plot(decile_df['Decile'], decile_df['Lift vs Random'], \n",
        "            marker='o', linewidth=2, markersize=8, label=model_name, color=colors[model_name])\n",
        "\n",
        "ax.axhline(y=1, color='gray', linestyle='--', linewidth=2, label='Random (1x)')\n",
        "ax.set_xlabel('Decile (1 = Highest Predicted Uplift)', fontsize=12)\n",
        "ax.set_ylabel('Lift vs Random Targeting', fontsize=12)\n",
        "ax.set_title('Model Comparison: Lift vs Random by Decile', fontsize=14)\n",
        "ax.set_xticks(range(1, 11))\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Highlight interpretation\n",
        "ax.annotate('Good model: High lift in early deciles,\\ndecreasing toward later deciles', \n",
        "            xy=(2, ax.get_ylim()[1] * 0.8), fontsize=10, \n",
        "            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Calibration Plot for Uplift\n",
        "\n",
        "**What is Uplift Calibration?**\n",
        "- A well-calibrated model: when it predicts 0.5% uplift, the actual uplift should be ~0.5%\n",
        "- If predicted 1% â†’ actual 2%, the model is **under-confident** (underestimates true effect)\n",
        "- If predicted 1% â†’ actual 0.5%, the model is **over-confident** (overestimates)\n",
        "\n",
        "**Why it matters:**\n",
        "- For budget allocation: if model says \"this segment has 2% uplift\", we need that to be accurate\n",
        "- Poorly calibrated models lead to wrong ROI calculations\n",
        "\n",
        "**Reading the plot:**\n",
        "- Perfect calibration = points on the diagonal line\n",
        "- Above diagonal = model under-predicts (actual > predicted)\n",
        "- Below diagonal = model over-predicts (actual < predicted)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def uplift_calibration(y_true, uplift_pred, treatment, n_bins=10):\n",
        "    \"\"\"\n",
        "    Calculate calibration data for uplift predictions.\n",
        "    \n",
        "    Groups predictions into bins and compares mean predicted vs actual uplift.\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'y': y_true,\n",
        "        'uplift_pred': uplift_pred,\n",
        "        'treatment': treatment\n",
        "    })\n",
        "    \n",
        "    # Bin by predicted uplift\n",
        "    df['bin'] = pd.qcut(df['uplift_pred'], q=n_bins, labels=False, duplicates='drop')\n",
        "    \n",
        "    results = []\n",
        "    for bin_idx in sorted(df['bin'].unique()):\n",
        "        subset = df[df['bin'] == bin_idx]\n",
        "        \n",
        "        treated = subset[subset['treatment'] == 1]\n",
        "        control = subset[subset['treatment'] == 0]\n",
        "        \n",
        "        mean_pred = subset['uplift_pred'].mean()\n",
        "        \n",
        "        if len(treated) > 0 and len(control) > 0:\n",
        "            actual_uplift = treated['y'].mean() - control['y'].mean()\n",
        "        else:\n",
        "            actual_uplift = np.nan\n",
        "            \n",
        "        results.append({\n",
        "            'bin': bin_idx + 1,\n",
        "            'mean_predicted': mean_pred,\n",
        "            'actual_uplift': actual_uplift,\n",
        "            'n_samples': len(subset)\n",
        "        })\n",
        "    \n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "print(\"âœ… Calibration function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate and plot calibration for each model\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "calibration_metrics = {}\n",
        "\n",
        "for ax, (model_name, uplift_pred) in zip(axes, models.items()):\n",
        "    cal_df = uplift_calibration(y_test, uplift_pred, t_test, n_bins=10)\n",
        "    \n",
        "    # Convert to percentage for readability\n",
        "    pred_pct = cal_df['mean_predicted'] * 100\n",
        "    actual_pct = cal_df['actual_uplift'] * 100\n",
        "    \n",
        "    # Scatter plot\n",
        "    ax.scatter(pred_pct, actual_pct, s=100, c=colors[model_name], alpha=0.7, edgecolors='black')\n",
        "    \n",
        "    # Perfect calibration line\n",
        "    min_val = min(pred_pct.min(), actual_pct.min())\n",
        "    max_val = max(pred_pct.max(), actual_pct.max())\n",
        "    ax.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label='Perfect calibration')\n",
        "    \n",
        "    # Calculate calibration error (Mean Absolute Error)\n",
        "    cal_error = np.abs(pred_pct - actual_pct).mean()\n",
        "    calibration_metrics[model_name] = cal_error\n",
        "    \n",
        "    ax.set_xlabel('Mean Predicted Uplift (%)')\n",
        "    ax.set_ylabel('Actual Uplift (%)')\n",
        "    ax.set_title(f'{model_name}\\nCalibration Error: {cal_error:.4f}%')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add regression line\n",
        "    z = np.polyfit(pred_pct, actual_pct, 1)\n",
        "    p = np.poly1d(z)\n",
        "    ax.plot(pred_pct.sort_values(), p(pred_pct.sort_values()), \n",
        "            color=colors[model_name], linestyle='-', linewidth=2, alpha=0.7)\n",
        "\n",
        "plt.suptitle('Uplift Calibration: Predicted vs Actual', fontsize=14, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary\n",
        "print(\"\\nðŸ“Š Calibration Error Summary (lower = better calibrated):\")\n",
        "for model, error in sorted(calibration_metrics.items(), key=lambda x: x[1]):\n",
        "    print(f\"  {model}: {error:.4f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combined calibration plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for model_name, uplift_pred in models.items():\n",
        "    cal_df = uplift_calibration(y_test, uplift_pred, t_test, n_bins=10)\n",
        "    pred_pct = cal_df['mean_predicted'] * 100\n",
        "    actual_pct = cal_df['actual_uplift'] * 100\n",
        "    \n",
        "    ax.scatter(pred_pct, actual_pct, s=100, c=colors[model_name], alpha=0.7, \n",
        "               edgecolors='black', label=model_name)\n",
        "    \n",
        "    # Connect points with lines\n",
        "    sorted_idx = pred_pct.argsort()\n",
        "    ax.plot(pred_pct.iloc[sorted_idx], actual_pct.iloc[sorted_idx], \n",
        "            color=colors[model_name], alpha=0.5, linewidth=1.5)\n",
        "\n",
        "# Perfect calibration line\n",
        "all_vals = []\n",
        "for uplift_pred in models.values():\n",
        "    cal_df = uplift_calibration(y_test, uplift_pred, t_test, n_bins=10)\n",
        "    all_vals.extend(cal_df['mean_predicted'] * 100)\n",
        "    all_vals.extend(cal_df['actual_uplift'] * 100)\n",
        "\n",
        "min_val, max_val = min(all_vals), max(all_vals)\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label='Perfect Calibration')\n",
        "\n",
        "ax.set_xlabel('Mean Predicted Uplift (%)', fontsize=12)\n",
        "ax.set_ylabel('Actual Uplift (%)', fontsize=12)\n",
        "ax.set_title('Uplift Calibration Comparison', fontsize=14)\n",
        "ax.legend(fontsize=10)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Add interpretation zone\n",
        "ax.fill_between([min_val, max_val], [min_val, max_val], [max_val, max_val*2], \n",
        "                alpha=0.1, color='green', label='_Under-predicting')\n",
        "ax.fill_between([min_val, max_val], [min_val*0.5, min_val], [min_val, max_val], \n",
        "                alpha=0.1, color='red', label='_Over-predicting')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Statistical Significance Testing\n",
        "\n",
        "**The Question:** Is T-Learner *significantly* better than X-Learner, or could the difference be due to random chance?\n",
        "\n",
        "**Approach: Bootstrap Hypothesis Testing**\n",
        "1. Resample the test set with replacement (B times)\n",
        "2. Calculate Qini coefficient for each model on each bootstrap sample\n",
        "3. Compute the difference (T-Learner Qini - X-Learner Qini) for each sample\n",
        "4. If 95% CI of difference excludes 0 â†’ statistically significant\n",
        "\n",
        "**Interpretation:**\n",
        "- p-value < 0.05: Difference is statistically significant\n",
        "- 95% CI doesn't include 0: One model is significantly better\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calc_qini_coefficient(y_true, uplift_pred, treatment, n_bins=100):\n",
        "    \"\"\"Calculate Qini coefficient (area between model and random Qini curves).\"\"\"\n",
        "    df = pd.DataFrame({\n",
        "        'y': y_true, \n",
        "        'uplift': uplift_pred, \n",
        "        'treatment': treatment\n",
        "    })\n",
        "    df = df.sort_values('uplift', ascending=False).reset_index(drop=True)\n",
        "    \n",
        "    n = len(df)\n",
        "    qini_values = []\n",
        "    \n",
        "    for i in range(1, n_bins + 1):\n",
        "        k = int(n * i / n_bins)\n",
        "        subset = df.iloc[:k]\n",
        "        \n",
        "        treated = subset[subset['treatment'] == 1]\n",
        "        control = subset[subset['treatment'] == 0]\n",
        "        \n",
        "        if len(treated) > 0 and len(control) > 0:\n",
        "            uplift = treated['y'].mean() - control['y'].mean()\n",
        "            qini = uplift * k\n",
        "        else:\n",
        "            qini = 0\n",
        "            \n",
        "        qini_values.append(qini)\n",
        "    \n",
        "    # Area under model's Qini curve\n",
        "    percentiles = np.linspace(0.01, 1, n_bins)\n",
        "    model_area = np.trapz(qini_values, percentiles)\n",
        "    \n",
        "    # Area under random Qini curve (triangle)\n",
        "    final_qini = qini_values[-1]\n",
        "    random_area = 0.5 * final_qini\n",
        "    \n",
        "    return model_area - random_area\n",
        "\n",
        "print(\"âœ… Qini coefficient function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_qini_comparison(y_true, uplift_a, uplift_b, treatment, \n",
        "                               n_bootstrap=1000, confidence_level=0.95):\n",
        "    \"\"\"\n",
        "    Bootstrap hypothesis test comparing two uplift models' Qini coefficients.\n",
        "    \n",
        "    Returns:\n",
        "    - diff_mean: Mean difference (A - B)\n",
        "    - diff_ci: Confidence interval for the difference\n",
        "    - p_value: Two-sided p-value for H0: no difference\n",
        "    \"\"\"\n",
        "    n = len(y_true)\n",
        "    diff_scores = []\n",
        "    \n",
        "    np.random.seed(42)\n",
        "    \n",
        "    for i in range(n_bootstrap):\n",
        "        # Bootstrap sample (with replacement)\n",
        "        idx = np.random.choice(n, size=n, replace=True)\n",
        "        \n",
        "        y_boot = y_true[idx]\n",
        "        uplift_a_boot = uplift_a[idx]\n",
        "        uplift_b_boot = uplift_b[idx]\n",
        "        t_boot = treatment[idx]\n",
        "        \n",
        "        # Calculate Qini for each model\n",
        "        qini_a = calc_qini_coefficient(y_boot, uplift_a_boot, t_boot)\n",
        "        qini_b = calc_qini_coefficient(y_boot, uplift_b_boot, t_boot)\n",
        "        \n",
        "        diff_scores.append(qini_a - qini_b)\n",
        "        \n",
        "        if (i + 1) % 200 == 0:\n",
        "            print(f\"  Bootstrap iteration {i+1}/{n_bootstrap}\")\n",
        "    \n",
        "    diff_scores = np.array(diff_scores)\n",
        "    \n",
        "    # Statistics\n",
        "    diff_mean = diff_scores.mean()\n",
        "    diff_std = diff_scores.std()\n",
        "    \n",
        "    # Confidence interval\n",
        "    alpha = 1 - confidence_level\n",
        "    ci_lower = np.percentile(diff_scores, alpha/2 * 100)\n",
        "    ci_upper = np.percentile(diff_scores, (1 - alpha/2) * 100)\n",
        "    \n",
        "    # P-value (two-sided): proportion of bootstrap samples where diff <= 0 (if mean > 0)\n",
        "    # or diff >= 0 (if mean < 0)\n",
        "    if diff_mean > 0:\n",
        "        p_value = 2 * (diff_scores <= 0).mean()\n",
        "    else:\n",
        "        p_value = 2 * (diff_scores >= 0).mean()\n",
        "    p_value = min(p_value, 1.0)  # Cap at 1\n",
        "    \n",
        "    return {\n",
        "        'diff_mean': diff_mean,\n",
        "        'diff_std': diff_std,\n",
        "        'ci_lower': ci_lower,\n",
        "        'ci_upper': ci_upper,\n",
        "        'p_value': p_value,\n",
        "        'diff_scores': diff_scores\n",
        "    }\n",
        "\n",
        "print(\"âœ… Bootstrap comparison function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run bootstrap comparison: T-Learner vs X-Learner\n",
        "print(\"=\" * 60)\n",
        "print(\"BOOTSTRAP HYPOTHESIS TEST: T-Learner vs X-Learner\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nH0: No difference in Qini coefficient between models\")\n",
        "print(\"H1: T-Learner has higher Qini coefficient than X-Learner\")\n",
        "print(\"\\nRunning 500 bootstrap iterations...\")\n",
        "\n",
        "result_tx = bootstrap_qini_comparison(\n",
        "    y_test, uplift_t, uplift_x, t_test, \n",
        "    n_bootstrap=500, confidence_level=0.95\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RESULTS: T-Learner vs X-Learner\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Mean Qini Difference (T - X): {result_tx['diff_mean']:.4f}\")\n",
        "print(f\"Standard Error: {result_tx['diff_std']:.4f}\")\n",
        "print(f\"95% Confidence Interval: [{result_tx['ci_lower']:.4f}, {result_tx['ci_upper']:.4f}]\")\n",
        "print(f\"P-value: {result_tx['p_value']:.4f}\")\n",
        "\n",
        "if result_tx['p_value'] < 0.05:\n",
        "    print(f\"\\nâœ… SIGNIFICANT at Î±=0.05: T-Learner is statistically better than X-Learner\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ NOT significant at Î±=0.05: Cannot conclude T-Learner is better\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run bootstrap comparison: T-Learner vs S-Learner\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"BOOTSTRAP HYPOTHESIS TEST: T-Learner vs S-Learner\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nRunning 500 bootstrap iterations...\")\n",
        "\n",
        "result_ts = bootstrap_qini_comparison(\n",
        "    y_test, uplift_t, uplift_s, t_test, \n",
        "    n_bootstrap=500, confidence_level=0.95\n",
        ")\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"RESULTS: T-Learner vs S-Learner\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Mean Qini Difference (T - S): {result_ts['diff_mean']:.4f}\")\n",
        "print(f\"Standard Error: {result_ts['diff_std']:.4f}\")\n",
        "print(f\"95% Confidence Interval: [{result_ts['ci_lower']:.4f}, {result_ts['ci_upper']:.4f}]\")\n",
        "print(f\"P-value: {result_ts['p_value']:.4f}\")\n",
        "\n",
        "if result_ts['p_value'] < 0.05:\n",
        "    print(f\"\\nâœ… SIGNIFICANT at Î±=0.05: T-Learner is statistically better than S-Learner\")\n",
        "else:\n",
        "    print(f\"\\nâš ï¸ NOT significant at Î±=0.05: Cannot conclude T-Learner is better\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize bootstrap distributions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# T-Learner vs X-Learner\n",
        "ax1 = axes[0]\n",
        "ax1.hist(result_tx['diff_scores'], bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No difference (H0)')\n",
        "ax1.axvline(x=result_tx['diff_mean'], color='green', linestyle='-', linewidth=2, \n",
        "            label=f'Mean diff: {result_tx[\"diff_mean\"]:.2f}')\n",
        "ax1.axvline(x=result_tx['ci_lower'], color='orange', linestyle=':', linewidth=2)\n",
        "ax1.axvline(x=result_tx['ci_upper'], color='orange', linestyle=':', linewidth=2, \n",
        "            label=f'95% CI: [{result_tx[\"ci_lower\"]:.2f}, {result_tx[\"ci_upper\"]:.2f}]')\n",
        "ax1.set_xlabel('Qini Difference (T-Learner - X-Learner)')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title(f'Bootstrap Distribution: T vs X\\np-value = {result_tx[\"p_value\"]:.4f}')\n",
        "ax1.legend(fontsize=9)\n",
        "\n",
        "# T-Learner vs S-Learner\n",
        "ax2 = axes[1]\n",
        "ax2.hist(result_ts['diff_scores'], bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No difference (H0)')\n",
        "ax2.axvline(x=result_ts['diff_mean'], color='green', linestyle='-', linewidth=2,\n",
        "            label=f'Mean diff: {result_ts[\"diff_mean\"]:.2f}')\n",
        "ax2.axvline(x=result_ts['ci_lower'], color='orange', linestyle=':', linewidth=2)\n",
        "ax2.axvline(x=result_ts['ci_upper'], color='orange', linestyle=':', linewidth=2,\n",
        "            label=f'95% CI: [{result_ts[\"ci_lower\"]:.2f}, {result_ts[\"ci_upper\"]:.2f}]')\n",
        "ax2.set_xlabel('Qini Difference (T-Learner - S-Learner)')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title(f'Bootstrap Distribution: T vs S\\np-value = {result_ts[\"p_value\"]:.4f}')\n",
        "ax2.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary & Key Takeaways\n",
        "\n",
        "### Advanced Evaluation Methods Covered:\n",
        "\n",
        "| Method | Purpose | Key Insight |\n",
        "|--------|---------|-------------|\n",
        "| **Decile Analysis** | Granular performance view | See uplift in each population segment |\n",
        "| **Calibration Plot** | Check prediction accuracy | Is predicted uplift reliable for ROI calculations? |\n",
        "| **Bootstrap Testing** | Statistical significance | Is model A *significantly* better than B? |\n",
        "\n",
        "### Business Implications:\n",
        "\n",
        "1. **Decile Analysis**: \n",
        "   - Target only the top deciles where uplift is highest\n",
        "   - Avoid bottom deciles which may have negative uplift (Sleeping Dogs)\n",
        "\n",
        "2. **Calibration**:\n",
        "   - Well-calibrated models enable accurate ROI projections\n",
        "   - Poorly calibrated â†’ misleading budget allocations\n",
        "\n",
        "3. **Statistical Significance**:\n",
        "   - Don't deploy a new model just because Qini is slightly higher\n",
        "   - Require statistical significance to justify implementation costs\n",
        "\n",
        "### Recommended Workflow:\n",
        "\n",
        "```\n",
        "1. Train multiple uplift models\n",
        "2. Compare with basic metrics (Qini, AUUC)\n",
        "3. Dive deeper with decile analysis\n",
        "4. Check calibration for deployment readiness\n",
        "5. Run bootstrap tests before finalizing model choice\n",
        "6. Deploy winner, monitor in production\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## ðŸš€ Production Pipeline: Save Artifacts\n",
        "\n",
        "Save evaluation results and visualizations for Streamlit dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "from pathlib import Path\n",
        "\n",
        "# Create directories\n",
        "project_root = Path('..').resolve()\n",
        "models_dir = project_root / 'models'\n",
        "viz_images_dir = project_root / 'visualizations' / 'images'\n",
        "viz_data_dir = project_root / 'visualizations' / 'data'\n",
        "\n",
        "for dir_path in [models_dir, viz_images_dir, viz_data_dir]:\n",
        "    dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Directories ready for saving artifacts\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save decile analysis data for all models\n",
        "print(\"Saving decile analysis data...\")\n",
        "\n",
        "decile_data_list = []\n",
        "for model_name, uplift_pred in models.items():\n",
        "    decile_df = uplift_by_decile(y_test, uplift_pred, t_test)\n",
        "    decile_df['Model'] = model_name\n",
        "    decile_data_list.append(decile_df)\n",
        "\n",
        "all_deciles_df = pd.concat(decile_data_list, ignore_index=True)\n",
        "all_deciles_df.to_csv(viz_data_dir / 'nb04_decile_analysis.csv', index=False)\n",
        "print(f\"âœ… Decile analysis saved: nb04_decile_analysis.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save calibration data\n",
        "print(\"Saving calibration data...\")\n",
        "\n",
        "calibration_data_list = []\n",
        "for model_name, uplift_pred in models.items():\n",
        "    cal_df = uplift_calibration(y_test, uplift_pred, t_test, n_bins=10)\n",
        "    cal_df['Model'] = model_name\n",
        "    calibration_data_list.append(cal_df)\n",
        "\n",
        "all_calibration_df = pd.concat(calibration_data_list, ignore_index=True)\n",
        "all_calibration_df.to_csv(viz_data_dir / 'nb04_calibration_data.csv', index=False)\n",
        "print(f\"âœ… Calibration data saved: nb04_calibration_data.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save bootstrap test results\n",
        "print(\"Saving bootstrap test results...\")\n",
        "\n",
        "bootstrap_results = pd.DataFrame({\n",
        "    'Comparison': ['T-Learner vs X-Learner', 'T-Learner vs S-Learner'],\n",
        "    'Mean_Difference': [result_tx['diff_mean'], result_ts['diff_mean']],\n",
        "    'Std_Error': [result_tx['diff_std'], result_ts['diff_std']],\n",
        "    'CI_Lower': [result_tx['ci_lower'], result_ts['ci_lower']],\n",
        "    'CI_Upper': [result_tx['ci_upper'], result_ts['ci_upper']],\n",
        "    'P_Value': [result_tx['p_value'], result_ts['p_value']],\n",
        "    'Significant': [result_tx['p_value'] < 0.05, result_ts['p_value'] < 0.05]\n",
        "})\n",
        "\n",
        "bootstrap_results.to_csv(viz_data_dir / 'nb04_bootstrap_results.csv', index=False)\n",
        "print(f\"âœ… Bootstrap results saved: nb04_bootstrap_results.csv\")\n",
        "bootstrap_results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save visualization images\n",
        "print(\"Saving visualization images...\")\n",
        "colors = {'T-Learner': '#3498db', 'S-Learner': '#e74c3c', 'X-Learner': '#2ecc71'}\n",
        "\n",
        "# 1. Decile Analysis - Lift vs Random\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "for model_name, uplift_pred in models.items():\n",
        "    decile_df = uplift_by_decile(y_test, uplift_pred, t_test)\n",
        "    ax.plot(decile_df['Decile'], decile_df['Lift vs Random'], \n",
        "            marker='o', linewidth=2, markersize=8, label=model_name, color=colors[model_name])\n",
        "\n",
        "ax.axhline(y=1, color='gray', linestyle='--', linewidth=2, label='Random (1x)')\n",
        "ax.set_xlabel('Decile (1 = Highest Predicted Uplift)')\n",
        "ax.set_ylabel('Lift vs Random Targeting')\n",
        "ax.set_title('Model Comparison: Lift vs Random by Decile')\n",
        "ax.set_xticks(range(1, 11))\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(viz_images_dir / 'nb04_decile_lift.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: nb04_decile_lift.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2. Calibration Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "for model_name, uplift_pred in models.items():\n",
        "    cal_df = uplift_calibration(y_test, uplift_pred, t_test, n_bins=10)\n",
        "    pred_pct = cal_df['mean_predicted'] * 100\n",
        "    actual_pct = cal_df['actual_uplift'] * 100\n",
        "    \n",
        "    ax.scatter(pred_pct, actual_pct, s=100, c=colors[model_name], alpha=0.7, \n",
        "               edgecolors='black', label=model_name)\n",
        "    sorted_idx = pred_pct.argsort()\n",
        "    ax.plot(pred_pct.iloc[sorted_idx], actual_pct.iloc[sorted_idx], \n",
        "            color=colors[model_name], alpha=0.5, linewidth=1.5)\n",
        "\n",
        "# Perfect calibration line\n",
        "all_vals = []\n",
        "for uplift_pred in models.values():\n",
        "    cal_df = uplift_calibration(y_test, uplift_pred, t_test, n_bins=10)\n",
        "    all_vals.extend(cal_df['mean_predicted'] * 100)\n",
        "    all_vals.extend(cal_df['actual_uplift'] * 100)\n",
        "\n",
        "min_val, max_val = min(all_vals), max(all_vals)\n",
        "ax.plot([min_val, max_val], [min_val, max_val], 'k--', linewidth=2, label='Perfect Calibration')\n",
        "\n",
        "ax.set_xlabel('Mean Predicted Uplift (%)')\n",
        "ax.set_ylabel('Actual Uplift (%)')\n",
        "ax.set_title('Uplift Calibration Comparison')\n",
        "ax.legend()\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(viz_images_dir / 'nb04_calibration.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: nb04_calibration.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3. Bootstrap Distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# T-Learner vs X-Learner\n",
        "ax1 = axes[0]\n",
        "ax1.hist(result_tx['diff_scores'], bins=50, color='#3498db', alpha=0.7, edgecolor='black')\n",
        "ax1.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No difference (H0)')\n",
        "ax1.axvline(x=result_tx['diff_mean'], color='green', linestyle='-', linewidth=2, \n",
        "            label=f'Mean diff: {result_tx[\"diff_mean\"]:.2f}')\n",
        "ax1.set_xlabel('Qini Difference (T-Learner - X-Learner)')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.set_title(f'Bootstrap: T vs X\\np-value = {result_tx[\"p_value\"]:.4f}')\n",
        "ax1.legend(fontsize=9)\n",
        "\n",
        "# T-Learner vs S-Learner\n",
        "ax2 = axes[1]\n",
        "ax2.hist(result_ts['diff_scores'], bins=50, color='#e74c3c', alpha=0.7, edgecolor='black')\n",
        "ax2.axvline(x=0, color='red', linestyle='--', linewidth=2, label='No difference (H0)')\n",
        "ax2.axvline(x=result_ts['diff_mean'], color='green', linestyle='-', linewidth=2,\n",
        "            label=f'Mean diff: {result_ts[\"diff_mean\"]:.2f}')\n",
        "ax2.set_xlabel('Qini Difference (T-Learner - S-Learner)')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.set_title(f'Bootstrap: T vs S\\np-value = {result_ts[\"p_value\"]:.4f}')\n",
        "ax2.legend(fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(viz_images_dir / 'nb04_bootstrap_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(f\"âœ… Saved: nb04_bootstrap_distributions.png\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"âœ… ALL NOTEBOOK 04 ARTIFACTS SAVED SUCCESSFULLY!\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "criteo-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
